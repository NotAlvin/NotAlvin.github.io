+++
title = 'Replicating Paper Performance'
date = 2024-05-02T00:42:24+08:00
draft = true
math = false
+++

## Introduction
Replicating the performance of scientific papers is vital for ensuring the integrity and reliability of research findings. This is especially relevant in the machine learning field, where with new Generative AI papers coming out every day validating the results of previous studies allow us to verify the robustness of their conclusions. By independently reproducing experimental procedures and analyses, data scientists can uncover potential errors, biases, or limitations in the original work, thereby strengthening the validity of the scientific knowledge base.

Moreover, replication fosters transparency and accountability within the scientific community, promoting trust and confidence in the reliability of research outcomes. Going back to the topic of our previous article on [Quantifying Agent Performance Optimization](https://notalvin.github.io/posts/agent-ablation/) today we'll be trying to replicate the outcomes of [AgentCoder](https://arxiv.org/abs/2312.13010), a recently published paper on Multi-Agent-based Code Generation with Iterative Testing and Optimisation.

## Experiment Design



## Conclusion


